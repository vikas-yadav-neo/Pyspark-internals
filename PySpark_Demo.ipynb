{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13631170-321c-4bea-af50-e287f2fa2564",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PySpark Demo Notebook\n",
    "\n",
    "This notebook provides practical examples covering PySpark fundamentals, including RDDs, DataFrames, SparkSession setup, partitioning, caching, joins, broadcast variables, accumulators, and dynamic resource allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df32c4a3-94b0-406a-bb11-24095bbd854b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup SparkSession\n",
    "\n",
    "Initialize SparkSession which serves as the entry point for DataFrame and SQL operations in Spark 2.x+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e65402a5-50d5-40a7-ae79-142d8ad26f5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AppName: Databricks Shell\nMaster: local[8]\nDynamic Allocation Enabled: true\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# build or retrieve a SparkSession with dynamic executor allocation\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"PySpark Demo\")  # name the Spark application\n",
    "    .master(\"local[*]\")  # run locally using all cores\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")  # enable dynamic scaling of executors\n",
    "    .config(\"spark.dynamicAllocation.initialExecutors\", \"2\")  # start with 2 executors\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"1\")  # allow scaling down to 1 executor\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"4\")  # allow scaling up to 4 executors\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"60s\")  # remove idle executors after 60s\n",
    "    .getOrCreate()  # create the session if not already existing\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext  # get the low-level SparkContext from the SparkSession\n",
    "\n",
    "# print out configuration details to verify\n",
    "print(f\"AppName: {sc.appName}\")\n",
    "print(f\"Master: {sc.master}\")\n",
    "print(f\"Dynamic Allocation Enabled: {spark.conf.get('spark.dynamicAllocation.enabled')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb9f1e1b-c569-4de7-8260-985365fa1535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## RDD: Resilient Distributed Datasets\n",
    "RDD is Spark's low-level immutable distributed collection. It supports fine-grained control over transformations and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0cc0c48-1783-466b-a8fc-e382e034518f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 3\nCollect: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "# Create an RDD from a Python list\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "rdd = sc.parallelize(data, numSlices=3)  # The parallelize() method is used to create an RDD (Resilient Distributed Dataset) from an existing Python collection (like a list or range).\n",
    "print(\"Number of partitions:\", rdd.getNumPartitions())\n",
    "print(\"Collect:\", rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f77277d7-848d-48ac-a3a4-92b139719ffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transformations & Actions\n",
    "\n",
    "- Transformations are lazy (e.g., `map`, `filter`).\n",
    "- Actions trigger execution (e.g., `collect`, `count`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "443de344-ce1a-454a-aabf-500e49880f95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('python', 1), ('spark', 4), ('brings', 1), ('apache', 1), ('fast', 1), ('runs', 1), ('in', 1), ('supports', 1), ('pyspark', 1), ('to', 1), ('memory', 1), ('and', 1), ('dataframe', 1), ('is', 1), ('rdd', 1), ('apis', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Example: word count using RDD operations\n",
    "text_data = sc.parallelize([\n",
    "    \"Apache Spark is fast\",\n",
    "    \"PySpark brings Python to Spark\",\n",
    "    \"Spark runs in memory\",\n",
    "    \"Spark supports RDD and DataFrame APIs\"\n",
    "])\n",
    "\n",
    "# Split lines into words\n",
    "words = text_data.flatMap(lambda line: line.split(\" \")) # flattens the result into a single list of words\n",
    "word_pairs = words.map(lambda w: (w.lower(), 1))\n",
    "counts = word_pairs.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "print(counts.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfc0688f-e2f0-4cdc-83bc-1009385d1cc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## DataFrames & Spark SQL\n",
    "High-level API for structured data. It provides schema-based transformations and SQL querying capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78e86f2d-60a1-4631-9cb1-c5d1b8572d35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+\n|age| id|   name|\n+---+---+-------+\n| 30|  1|  Alice|\n| 25|  2|    Bob|\n| 35|  3|Charlie|\n+---+---+-------+\n\nroot\n |-- age: long (nullable = true)\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Assuming SparkSession 'spark' is already created\n",
    "\n",
    "json_data = [\n",
    "    {\"id\": 1, \"name\": \"Alice\", \"age\": 30},\n",
    "    {\"id\": 2, \"name\": \"Bob\", \"age\": 25},\n",
    "    {\"id\": 3, \"name\": \"Charlie\", \"age\": 35}\n",
    "]\n",
    "\n",
    "# Directly create a DataFrame from list of dicts\n",
    "df = spark.createDataFrame(json_data)\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddba1046-c4f9-474f-816d-8d63e31495b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n|   name|age|\n+-------+---+\n|  Alice| 30|\n|Charlie| 35|\n+-------+---+\n\n+---+-----+\n|age|count|\n+---+-----+\n| 30|    1|\n| 25|    1|\n| 35|    1|\n+---+-----+\n\n+-----+\n| name|\n+-----+\n|Alice|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# DataFrame operations\n",
    "df.select(\"name\", \"age\").filter(df.age > 28).show()\n",
    "\n",
    "df.groupBy(\"age\").count().show()\n",
    "\n",
    "# Spark SQL\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "spark.sql(\"SELECT name FROM people WHERE age BETWEEN 26 AND 34\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f47f8f69-d9c5-43f6-bb50-91ac5ec216fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Partitioning\n",
    "Spark divides data into partitions. Control partitions with `repartition` and `coalesce`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cd47fe6-c966-4b60-8ec4-ab6c08462702",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original partitions: 4\nRepartitioned (6): 6\nCoalesced (2): 2\n"
     ]
    }
   ],
   "source": [
    "rdd_part = sc.parallelize(range(20), 4)\n",
    "print(\"Original partitions:\", rdd_part.getNumPartitions())\n",
    "\n",
    "rdd_repart = rdd_part.repartition(6)\n",
    "print(\"Repartitioned (6):\", rdd_repart.getNumPartitions())\n",
    "\n",
    "rdd_coalesce = rdd_part.coalesce(2)\n",
    "print(\"Coalesced (2):\", rdd_coalesce.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4680262-37f1-478c-9f9c-fb34a771d4bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Caching & Persistence\n",
    "Persist datasets in memory for faster reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaf457e9-5af4-40a2-aaec-265d2b7836b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n20\nOut[7]: PythonRDD[25] at RDD at PythonRDD.scala:58"
     ]
    }
   ],
   "source": [
    "# Cache the RDD in memory for faster reuse\n",
    "rdd_cached = rdd_part.cache()\n",
    "\n",
    "# First action triggers computation and caches the RDD in memory\n",
    "print(rdd_cached.count())    # Computes and caches the RDD, then returns count\n",
    "\n",
    "# Second action uses cached data, so no recomputation happens\n",
    "print(rdd_cached.count())    # Returns count quickly from cached data\n",
    "\n",
    "# Remove the RDD from cache to free up memory when no longer needed\n",
    "rdd_cached.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20445119-d78a-4b1b-8a83-84d94759ca55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Broadcast Variables & Accumulators\n",
    "Share read-only data efficiently and aggregate values across tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "163df058-a419-4d49-af49-f8bcaf64af1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6]\nEven count via accumulator: 5\n"
     ]
    }
   ],
   "source": [
    "# Broadcast variable: efficiently share a read-only variable with all worker nodes\n",
    "broadcast_list = sc.broadcast([2, 4, 6])\n",
    "print(broadcast_list.value)  # Access the broadcasted value on the driver\n",
    "\n",
    "# Accumulator: a variable used to aggregate information (like counters) across tasks\n",
    "acc = sc.accumulator(0)  # Initialize accumulator with 0\n",
    "\n",
    "def add_if_even(x):\n",
    "    # If number is even, increment the accumulator by 1\n",
    "    if x % 2 == 0:\n",
    "        acc.add(1)\n",
    "    return x\n",
    "\n",
    "# Create an RDD of numbers from 0 to 9\n",
    "rdd_nums = sc.parallelize(range(10))\n",
    "\n",
    "# Run the function on each element of the RDD (side effect updates accumulator)\n",
    "rdd_nums.foreach(add_if_even)\n",
    "\n",
    "# Print the total count of even numbers computed by the accumulator\n",
    "print(\"Even count via accumulator:\", acc.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62c1d8b8-4fb5-4373-87d9-b590756ab990",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Join Strategies\n",
    "Demonstrate broadcast hash join vs sort-merge join using DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ceffcef7-e53b-4bdb-858b-f0299fcf21c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n'Join UsingJoin(Inner,Buffer(key))\n:- Project [id#61L AS key#63L]\n:  +- Range (1, 1001, step=1, splits=Some(8))\n+- ResolvedHint (strategy=broadcast)\n   +- LogicalRDD [key#65L, value#66], false\n\n== Analyzed Logical Plan ==\nkey: bigint, value: string\nProject [key#63L, value#66]\n+- Join Inner, (key#63L = key#65L)\n   :- Project [id#61L AS key#63L]\n   :  +- Range (1, 1001, step=1, splits=Some(8))\n   +- ResolvedHint (strategy=broadcast)\n      +- LogicalRDD [key#65L, value#66], false\n\n== Optimized Logical Plan ==\nProject [key#63L, value#66]\n+- Join Inner, (key#63L = key#65L), rightHint=(strategy=broadcast)\n   :- Project [id#61L AS key#63L]\n   :  +- Range (1, 1001, step=1, splits=Some(8))\n   +- Filter isnotnull(key#65L)\n      +- LogicalRDD [key#65L, value#66], false\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [key#63L, value#66]\n   +- BroadcastHashJoin [key#63L], [key#65L], Inner, BuildRight, false, true\n      :- Project [id#61L AS key#63L]\n      :  +- Range (1, 1001, step=1, splits=8)\n      +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=168]\n         +- Filter isnotnull(key#65L)\n            +- Scan ExistingRDD[key#65L,value#66]\n\n== Parsed Logical Plan ==\n'Join UsingJoin(Inner,Buffer(key))\n:- Project [id#61L AS key#63L]\n:  +- Range (1, 1001, step=1, splits=Some(8))\n+- LogicalRDD [key#65L, value#66], false\n\n== Analyzed Logical Plan ==\nkey: bigint, value: string\nProject [key#63L, value#66]\n+- Join Inner, (key#63L = key#65L)\n   :- Project [id#61L AS key#63L]\n   :  +- Range (1, 1001, step=1, splits=Some(8))\n   +- LogicalRDD [key#65L, value#66], false\n\n== Optimized Logical Plan ==\nProject [key#63L, value#66]\n+- Join Inner, (key#63L = key#65L)\n   :- Project [id#61L AS key#63L]\n   :  +- Range (1, 1001, step=1, splits=Some(8))\n   +- Filter isnotnull(key#65L)\n      +- LogicalRDD [key#65L, value#66], false\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [key#63L, value#66]\n   +- BroadcastHashJoin [key#63L], [key#65L], Inner, BuildLeft, false, true\n      :- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=235]\n      :  +- Project [id#61L AS key#63L]\n      :     +- Range (1, 1001, step=1, splits=8)\n      +- Filter isnotnull(key#65L)\n         +- Scan ExistingRDD[key#65L,value#66]\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Create a large DataFrame with keys from 1 to 1000\n",
    "df_large = spark.range(1, 1001).withColumnRenamed(\"id\", \"key\")\n",
    "\n",
    "# Create a small DataFrame with 20 rows and key-value pairs\n",
    "df_small = spark.createDataFrame([(i, f\"val_{i}\") for i in range(1, 21)], [\"key\", \"value\"])\n",
    "\n",
    "# Broadcast hash join:\n",
    "# Explicitly broadcast the small DataFrame to all worker nodes to optimize the join\n",
    "broadcast_join = df_large.join(broadcast(df_small), on=\"key\")\n",
    "broadcast_join.explain(True)  # Print detailed physical and logical plans\n",
    "\n",
    "# Sort-merge join (default join strategy):\n",
    "# This join does not broadcast and uses a sort-merge join which is good for large datasets\n",
    "merge_join = df_large.join(df_small, on=\"key\")\n",
    "merge_join.explain(True)  # Print detailed physical and logical plans"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark_Demo",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}