{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Jm7DhP4WI_6",
        "outputId": "2e9338b8-7d83-4f52-ec6b-bbfaa2554bdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark Demo Notebook\n",
        "\n",
        "This notebook provides practical examples covering PySpark fundamentals, including RDDs, DataFrames, SparkSession setup, partitioning, caching, joins, broadcast variables, accumulators, and dynamic resource allocation."
      ],
      "metadata": {
        "id": "PbbHkcOqWSnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup SparkSession\n",
        "\n",
        "Initialize SparkSession which serves as the entry point for DataFrame and SQL operations in Spark 2.x+."
      ],
      "metadata": {
        "id": "TZ1ZfHi2WWJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"PySpark Demo\")\n",
        "    .master(\"local[*]\")\n",
        "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
        "    .config(\"spark.dynamicAllocation.initialExecutors\", \"2\")\n",
        "    .config(\"spark.dynamicAllocation.minExecutors\", \"1\")\n",
        "    .config(\"spark.dynamicAllocation.maxExecutors\", \"4\")\n",
        "    .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"60s\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "print(f\"AppName: {sc.appName}\")\n",
        "print(f\"Master: {sc.master}\")\n",
        "print(f\"Dynamic Allocation Enabled: {spark.conf.get('spark.dynamicAllocation.enabled')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q5DFynyWPG4",
        "outputId": "b20026df-6277-4426-abbb-abd5e661555e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AppName: PySpark Demo\n",
            "Master: local[*]\n",
            "Dynamic Allocation Enabled: true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## RDD: Resilient Distributed Datasets\n",
        "RDD is Spark's low-level immutable distributed collection. It supports fine-grained control over transformations and actions."
      ],
      "metadata": {
        "id": "GfN8R0C2Wcn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "rdd = sc.parallelize(data, numSlices=3)\n",
        "print(\"Number of partitions:\", rdd.getNumPartitions())\n",
        "print(\"Collect:\", rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XhQCrbUWaiM",
        "outputId": "0dbeb646-aed1-4cd1-eb41-e9b3669e9805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of partitions: 3\n",
            "Collect: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformations & Actions\n",
        "\n",
        "- Transformations are lazy (e.g., `map`, `filter`).\n",
        "- Actions trigger execution (e.g., `collect`, `count`)."
      ],
      "metadata": {
        "id": "t5rX5sffWhVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = sc.parallelize([\n",
        "    \"Apache Spark is fast\",\n",
        "    \"PySpark brings Python to Spark\",\n",
        "    \"Spark runs in memory\",\n",
        "    \"Spark supports RDD and DataFrame APIs\"\n",
        "])\n",
        "\n",
        "words = text_data.flatMap(lambda line: line.split(\" \"))\n",
        "word_pairs = words.map(lambda w: (w.lower(), 1))\n",
        "counts = word_pairs.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "print(counts.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YrPZc1TWfde",
        "outputId": "37da3438-f801-44b6-a78a-cfdf67eba0d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('apache', 1), ('fast', 1), ('brings', 1), ('python', 1), ('to', 1), ('runs', 1), ('memory', 1), ('supports', 1), ('rdd', 1), ('and', 1), ('dataframe', 1), ('spark', 4), ('is', 1), ('pyspark', 1), ('in', 1), ('apis', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## DataFrames & Spark SQL\n",
        "High-level API for structured data. It provides schema-based transformations and SQL querying capabilities."
      ],
      "metadata": {
        "id": "A3bCwFHwWnHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "json_data = [\n",
        "    {\"id\": 1, \"name\": \"Alice\", \"age\": 30},\n",
        "    {\"id\": 2, \"name\": \"Bob\", \"age\": 25},\n",
        "    {\"id\": 3, \"name\": \"Charlie\", \"age\": 35}\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(json_data)\n",
        "\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hQwPDQ3Wo2R",
        "outputId": "7595cdd2-01e0-4d34-b45a-7d66ae8378f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-------+\n",
            "|age| id|   name|\n",
            "+---+---+-------+\n",
            "| 30|  1|  Alice|\n",
            "| 25|  2|    Bob|\n",
            "| 35|  3|Charlie|\n",
            "+---+---+-------+\n",
            "\n",
            "root\n",
            " |-- age: long (nullable = true)\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"name\", \"age\").filter(df.age > 28).show()\n",
        "\n",
        "df.groupBy(\"age\").count().show()\n",
        "\n",
        "df.createOrReplaceTempView(\"people\")\n",
        "spark.sql(\"SELECT name FROM people WHERE age BETWEEN 26 AND 34\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAx4uRJ8Wquo",
        "outputId": "fc937219-ad64-434d-907d-8e6917167cfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+\n",
            "|   name|age|\n",
            "+-------+---+\n",
            "|  Alice| 30|\n",
            "|Charlie| 35|\n",
            "+-------+---+\n",
            "\n",
            "+---+-----+\n",
            "|age|count|\n",
            "+---+-----+\n",
            "| 30|    1|\n",
            "| 25|    1|\n",
            "| 35|    1|\n",
            "+---+-----+\n",
            "\n",
            "+-----+\n",
            "| name|\n",
            "+-----+\n",
            "|Alice|\n",
            "+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Partitioning\n",
        "Spark divides data into partitions. Control partitions with `repartition` and `coalesce`."
      ],
      "metadata": {
        "id": "pbvbsXDDWvYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_part = sc.parallelize(range(20), 4)\n",
        "print(\"Original partitions:\", rdd_part.getNumPartitions())\n",
        "\n",
        "rdd_repart = rdd_part.repartition(6)\n",
        "print(\"Repartitioned (6):\", rdd_repart.getNumPartitions())\n",
        "\n",
        "rdd_coalesce = rdd_part.coalesce(2)\n",
        "print(\"Coalesced (2):\", rdd_coalesce.getNumPartitions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76qh-1OcWsi6",
        "outputId": "5474d67d-babf-4c33-986f-fc34d9a69b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original partitions: 4\n",
            "Repartitioned (6): 6\n",
            "Coalesced (2): 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Caching & Persistence\n",
        "Persist datasets in memory for faster reuse."
      ],
      "metadata": {
        "id": "xf-m0WMcWzo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_cached = rdd_part.cache()\n",
        "\n",
        "print(rdd_cached.count())\n",
        "\n",
        "print(rdd_cached.count())    # Returns count quickly from cached data\n",
        "\n",
        "rdd_cached.unpersist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G73x6HRiWxw7",
        "outputId": "30a7d387-243b-4419-c4f2-61380ce366cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "20\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[97] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Broadcast Variables & Accumulators\n",
        "Share read-only data efficiently and aggregate values across tasks."
      ],
      "metadata": {
        "id": "qWB9X5KmW5RE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "broadcast_list = sc.broadcast([2, 4, 6])\n",
        "print(broadcast_list.value)\n",
        "\n",
        "acc = sc.accumulator(0)\n",
        "\n",
        "def add_if_even(x):\n",
        "    if x % 2 == 0:\n",
        "        acc.add(1)\n",
        "    return x\n",
        "\n",
        "rdd_nums = sc.parallelize(range(10))\n",
        "\n",
        "rdd_nums.foreach(add_if_even)\n",
        "\n",
        "print(\"Even count via accumulator:\", acc.value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBlVXeSOW2Qx",
        "outputId": "70505b47-f7a3-46ad-82de-bc356cfb989d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4, 6]\n",
            "Even count via accumulator: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Join Strategies\n",
        "Demonstrate broadcast hash join vs sort-merge join using DataFrames."
      ],
      "metadata": {
        "id": "KY4rclp4XQ5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "df_large = spark.range(1, 1001).withColumnRenamed(\"id\", \"key\")\n",
        "\n",
        "df_small = spark.createDataFrame([(i, f\"val_{i}\") for i in range(1, 21)], [\"key\", \"value\"])\n",
        "\n",
        "broadcast_join = df_large.join(broadcast(df_small), on=\"key\")\n",
        "broadcast_join.explain(True)\n",
        "\n",
        "merge_join = df_large.join(df_small, on=\"key\")\n",
        "merge_join.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtmheHP-XLu3",
        "outputId": "c818b8a1-9417-49cb-bea6-e747c433c12b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [key])\n",
            ":- Project [id#230L AS key#232L]\n",
            ":  +- Range (1, 1001, step=1, splits=Some(2))\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- LogicalRDD [key#234L, value#235], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "key: bigint, value: string\n",
            "Project [key#232L, value#235]\n",
            "+- Join Inner, (key#232L = key#234L)\n",
            "   :- Project [id#230L AS key#232L]\n",
            "   :  +- Range (1, 1001, step=1, splits=Some(2))\n",
            "   +- ResolvedHint (strategy=broadcast)\n",
            "      +- LogicalRDD [key#234L, value#235], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [key#232L, value#235]\n",
            "+- Join Inner, (key#232L = key#234L), rightHint=(strategy=broadcast)\n",
            "   :- Project [id#230L AS key#232L]\n",
            "   :  +- Range (1, 1001, step=1, splits=Some(2))\n",
            "   +- Filter isnotnull(key#234L)\n",
            "      +- LogicalRDD [key#234L, value#235], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [key#232L, value#235]\n",
            "   +- BroadcastHashJoin [key#232L], [key#234L], Inner, BuildRight, false\n",
            "      :- Project [id#230L AS key#232L]\n",
            "      :  +- Range (1, 1001, step=1, splits=2)\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=463]\n",
            "         +- Filter isnotnull(key#234L)\n",
            "            +- Scan ExistingRDD[key#234L,value#235]\n",
            "\n",
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [key])\n",
            ":- Project [id#230L AS key#232L]\n",
            ":  +- Range (1, 1001, step=1, splits=Some(2))\n",
            "+- LogicalRDD [key#234L, value#235], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "key: bigint, value: string\n",
            "Project [key#232L, value#235]\n",
            "+- Join Inner, (key#232L = key#234L)\n",
            "   :- Project [id#230L AS key#232L]\n",
            "   :  +- Range (1, 1001, step=1, splits=Some(2))\n",
            "   +- LogicalRDD [key#234L, value#235], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [key#232L, value#235]\n",
            "+- Join Inner, (key#232L = key#234L)\n",
            "   :- Project [id#230L AS key#232L]\n",
            "   :  +- Range (1, 1001, step=1, splits=Some(2))\n",
            "   +- Filter isnotnull(key#234L)\n",
            "      +- LogicalRDD [key#234L, value#235], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [key#232L, value#235]\n",
            "   +- BroadcastHashJoin [key#232L], [key#234L], Inner, BuildLeft, false\n",
            "      :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=490]\n",
            "      :  +- Project [id#230L AS key#232L]\n",
            "      :     +- Range (1, 1001, step=1, splits=2)\n",
            "      +- Filter isnotnull(key#234L)\n",
            "         +- Scan ExistingRDD[key#234L,value#235]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, concat_ws, floor, rand, when, explode, sequence, lit\n",
        "\n",
        "credits_data = [\n",
        "    (101, \"Tom Hardy\", \"Actor\"),\n",
        "    (101, \"Tom Hardy\", \"Actor\"),\n",
        "    (101, \"Tom Hardy\", \"Actor\"),\n",
        "    (101, \"Tom Hardy\", \"Actor\"),\n",
        "    (101, \"Tom Hardy\", \"Actor\"),\n",
        "    (101, \"Tom Hardy\", \"Actor\"),\n",
        "    (101, \"Tom Hardy\", \"Actor\"),\n",
        "    (101, \"Tom Hardy\", \"Actor\"),\n",
        "    (101, \"Tom Hardy\", \"Actor\"),\n",
        "    (101, \"Tom Hardy\", \"Actor\"),\n",
        "    (101, \"Tom Hardy\", \"Actor\"),\n",
        "    (102, \"Emma Stone\", \"Actress\"),\n",
        "    (103, \"Chris Evans\", \"Actor\"),\n",
        "    (104, \"Scarlett Johansson\", \"Actress\"),\n",
        "    (105, \"Mark Ruffalo\", \"Actor\"),\n",
        "]\n",
        "credits_df = spark.createDataFrame(credits_data, [\"id\", \"name\", \"role\"])\n",
        "\n",
        "titles_data = [\n",
        "    (101, \"Inception\", \"movie\", 2010),\n",
        "    (102, \"La La Land\", \"movie\", 2016),\n",
        "    (103, \"Captain America\", \"movie\", 2011),\n",
        "    (104, \"Lucy\", \"movie\", 2014),\n",
        "    (105, \"Hulk\", \"movie\", 2003)\n",
        "]\n",
        "titles_df = spark.createDataFrame(titles_data, [\"id\", \"title\", \"type\", \"release_year\"])\n",
        "\n",
        "skewed_keys_df = credits_df.groupBy(\"id\").count().filter(\"count > 5\")\n",
        "skewed_ids = [row[\"id\"] for row in skewed_keys_df.collect()]\n",
        "\n",
        "credits_salted = credits_df.withColumn(\"original_id\", col(\"id\")).withColumn(\n",
        "    \"id_salted\",\n",
        "    when(\n",
        "        col(\"id\").isin(skewed_ids),\n",
        "        concat_ws(\"_\", col(\"id\"), floor(rand(seed=68) * 10).cast(\"int\"))\n",
        "    ).otherwise(col(\"id\"))\n",
        ").alias(\"credits\")\n",
        "\n",
        "titles_skewed = titles_df.filter(col(\"id\").isin(skewed_ids))\n",
        "titles_not_skewed = titles_df.filter(~col(\"id\").isin(skewed_ids))\n",
        "\n",
        "titles_salted = titles_skewed.withColumn(\n",
        "    \"salt\", explode(sequence(lit(0), lit(9)))\n",
        ").withColumn(\n",
        "    \"id_salted\", concat_ws(\"_\", col(\"id\"), col(\"salt\"))\n",
        ").drop(\"salt\")\n",
        "\n",
        "titles_not_skewed = titles_not_skewed.withColumn(\"id_salted\", col(\"id\"))\n",
        "\n",
        "titles_salted_final = titles_salted.unionByName(titles_not_skewed).alias(\"titles\")\n",
        "\n",
        "joined_df = credits_salted.join(\n",
        "    titles_salted_final,\n",
        "    on=\"id_salted\",\n",
        "    how=\"inner\"\n",
        ").select(\n",
        "    col(\"credits.original_id\").alias(\"credit_id\"),\n",
        "    col(\"credits.name\"),\n",
        "    col(\"credits.role\"),\n",
        "    col(\"titles.title\"),\n",
        "    col(\"titles.type\"),\n",
        "    col(\"titles.release_year\")\n",
        ")\n",
        "\n",
        "joined_df.show(20, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6d0859nXSVx",
        "outputId": "3e1cdaaa-ede6-4566-a6e1-ddab84d4c574"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------------+-------+---------------+-----+------------+\n",
            "|credit_id|name              |role   |title          |type |release_year|\n",
            "+---------+------------------+-------+---------------+-----+------------+\n",
            "|101      |Tom Hardy         |Actor  |Inception      |movie|2010        |\n",
            "|101      |Tom Hardy         |Actor  |Inception      |movie|2010        |\n",
            "|101      |Tom Hardy         |Actor  |Inception      |movie|2010        |\n",
            "|101      |Tom Hardy         |Actor  |Inception      |movie|2010        |\n",
            "|101      |Tom Hardy         |Actor  |Inception      |movie|2010        |\n",
            "|101      |Tom Hardy         |Actor  |Inception      |movie|2010        |\n",
            "|101      |Tom Hardy         |Actor  |Inception      |movie|2010        |\n",
            "|101      |Tom Hardy         |Actor  |Inception      |movie|2010        |\n",
            "|101      |Tom Hardy         |Actor  |Inception      |movie|2010        |\n",
            "|101      |Tom Hardy         |Actor  |Inception      |movie|2010        |\n",
            "|101      |Tom Hardy         |Actor  |Inception      |movie|2010        |\n",
            "|102      |Emma Stone        |Actress|La La Land     |movie|2016        |\n",
            "|103      |Chris Evans       |Actor  |Captain America|movie|2011        |\n",
            "|104      |Scarlett Johansson|Actress|Lucy           |movie|2014        |\n",
            "|105      |Mark Ruffalo      |Actor  |Hulk           |movie|2003        |\n",
            "+---------+------------------+-------+---------------+-----+------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}